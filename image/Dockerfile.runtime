# Annotated reference: Original runtime Dockerfile mapped to the new universal image
# Purpose: Show where each part of the old runtime corresponds in the universal image
# Notes:
# - Universal base image: quay.io/opendatahub/workbench-images:cuda-jupyter-minimal-ubi9-python-3.12-2025a_20250903
#   Provides UBI9, Python 3.12, CUDA 12.8 runtime, Jupyter minimal stack, Elyra addons, oc CLI, PDF deps.
# - Universal top-level adds: RDMA/OFED packages and Python training stack (torch 2.8.0 cu128, flash-attn 2.8.3, HF, Deepspeed, etc.)
# - Universal entry behavior: ENTRYPOINT wrapper selects start-notebook.sh by default or execs provided command (runtime)

## [runtime] Build args (Python/base tag)
## [universal] Not used. Python 3.12 and base tag are inherited from the minimal base image.
ARG IMAGE_TAG=9.6-1755735361
ARG PYTHON_VERSION=312

## [runtime] Base image is UBI9 Python
## [universal] Replaced by minimal workbench base (see universal Dockerfile FROM). No direct FROM ubi9/python used.
FROM registry.access.redhat.com/ubi9/python-${PYTHON_VERSION}:${IMAGE_TAG}

## [runtime] Labels
## [universal] Universal image has its own labels/comments; these specific labels are not copied as-is.
LABEL name="training:py312-cuda128-torch280" \
      summary="CUDA 12.8 Python 3.12 PyTorch 2.8.0 image based on UBI9 for Training" \
      description="CUDA 12.8 Python 3.12 PyTorch 2.8.0 image based on UBI9 for Training" \
      io.k8s.display-name="CUDA 12.8 Python 3.12 PyTorch 2.8.0 base image for Training" \
      io.k8s.description="CUDA 12.8 Python 3.12 PyTorch 2.8.0 image based on UBI9 for Training" \
      authoritative-source-url="https://github.com/opendatahub-io/distributed-workloads"

## [runtime] Copy license
## [universal] Not copied; minimal base image licensing is handled upstream.
COPY LICENSE.md /licenses/cuda-license.md

## [runtime] Switch to root and set /app
## [universal] Minimal base uses /opt/app-root; universal aligns to base conventions.
USER 0
WORKDIR /app

## [runtime] Upgrade requests via pip
## [universal] We avoid ad-hoc upgrades of base libraries to not drift base; requests bump is omitted.
RUN pip install --no-cache-dir --upgrade requests==2.32.3

## [runtime] CUDA environment and toolchain install
## [universal] CUDA 12.8 runtime is provided by the minimal base image. We avoid re-installing cudart/compat by default.
##           nvcc can be optionally installed in universal (commented ARG in Dockerfile) if needed for build-time ops.
WORKDIR /opt/app-root/bin
ENV CUDA_VERSION=12.8.0 \
    NVIDIA_REQUIRE_CUDA="cuda>=12.8 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=570,driver<571 brand=unknown,driver>=570,driver<571 brand=nvidia,driver>=570,driver<571 brand=nvidiartx,driver>=570,driver<571 brand=geforce,driver>=570,driver<571 brand=geforcertx,driver>=570,driver<571 brand=quadro,driver>=570,driver<571 brand=quadrortx,driver>=570,driver<571 brand=titan,driver>=570,driver<571 brand=titanrtx,driver>=570,driver<571" \
    NV_CUDA_LIB_VERSION=12.8.0-1 \
    NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility \
    NV_CUDA_CUDART_VERSION=12.8.57-1 \
    NV_CUDA_COMPAT_VERSION=3:570.172.08-1.el9 \
    NV_CUDA_NVCC_VERSION=12.8.61-1

## [runtime] Enable CUDA repo and install cudart/compat/nvcc
## [universal] Provided by base (runtime libs). nvcc is optional in universal (commented path). Avoid duplicate installs.
RUN dnf config-manager \
    --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/cuda-rhel9.repo \
 && dnf install -y \
     --disablerepo=rhel-9-for-x86_64-baseos-rpms \
     --disablerepo=rhel-9-for-x86_64-appstream-rpms \
     cuda-cudart-12-8-${NV_CUDA_CUDART_VERSION} \
     cuda-compat-12-8-${NV_CUDA_COMPAT_VERSION} \
     cuda-nvcc-12-8-${NV_CUDA_NVCC_VERSION} \
 && echo "/usr/local/nvidia/lib" >> /etc/ld.so.conf.d/nvidia.conf \
 && echo "/usr/local/nvidia/lib64" >> /etc/ld.so.conf.d/nvidia.conf \
 && dnf clean all

ENV CUDA_HOME="/usr/local/cuda" \
 PATH="/usr/local/nvidia/bin:${CUDA_HOME}/bin:${PATH}" \
 LD_LIBRARY_PATH="/usr/local/nvidia/lib:/usr/local/nvidia/lib64:$CUDA_HOME/lib64:$CUDA_HOME/extras/CUPTI/lib64:$LD_LIBRARY_PATH"

## [runtime] IB/RDMA repos and packages
## [universal] Migrated to universal OS layer; we enable Mellanox repo and install libibverbs-utils, rdma-core, etc.
RUN dnf config-manager \
        --add-repo https://linux.mellanox.com/public/repo/mlnx_ofed/latest/rhel9.5/mellanox_mlnx_ofed.repo 

RUN dnf install -y --disablerepo="*" --enablerepo="cuda-rhel9-x86_64,mlnx_ofed_24.10-1.1.4.0_base,ubi-9-appstream-rpms,ubi-9-baseos-rpms" \
        libibverbs-utils \
        infiniband-diags \
        libibumad3 \
        librdmacm \
        librdmacm-utils \
        rdma-core \
        mlnx-tools \
    && dnf clean all \
    && rm -rf /var/cache/dnf/*

## [runtime] Python deps via micropipenv + Pipfile.lock
## [universal] Replaced with explicit pip installs to avoid conflicting with minimal’s uv-locked stack.
##           We omit micropipenv and Pipfile.lock; we install the runtime training packages directly.
# Install micropipenv
aRUN pip install --no-cache-dir -U "micropipenv[toml]"
# Copy Pipfile.lock and install
COPY Pipfile.lock ./
RUN micropipenv install -- --no-cache-dir && \
    rm -f ./Pipfile.lock && \
    chmod -R g+w /opt/app-root/lib/python3.12/site-packages && \
    fix-permissions /opt/app-root -P

## [runtime] Flash Attention and NVIDIA CUDA wheels
## [universal] Provided similarly with pip in universal Dockerfile (torch 2.8.0 cu128 + flash-attn 2.8.3 + nvidia-* cu12.8 wheels).
RUN pip install wheel
RUN pip install --no-cache-dir flash-attn==2.8.3 --no-build-isolation

RUN pip install \
    nvidia-nccl-cu12==2.27.3 \
    nvidia-cublas-cu12==12.8.4.1 \
    nvidia-cuda-cupti-cu12==12.8.90 \
    nvidia-cuda-nvrtc-cu12==12.8.93 \
    nvidia-cuda-runtime-cu12==12.8.90 \
    nvidia-cudnn-cu12==9.10.2.21 \
    nvidia-cufft-cu12==11.3.3.83 \
    nvidia-cufile-cu12==1.13.1.3 \
    nvidia-curand-cu12==10.3.9.90 \
    nvidia-cusolver-cu12==11.7.3.90 \
    nvidia-cusparse-cu12==12.5.8.93 \
    nvidia-cusparselt-cu12==0.7.1 \
    nvidia-nvjitlink-cu12==12.8.93 \
    nvidia-nvtx-cu12==12.8.90 \
 && fix-permissions /opt/app-root -P

## [runtime] Restore user and workspace
## [universal] Same principle; universal uses USER 1001 and /opt/app-root/src.
USER 1001
WORKDIR /opt/app-root/src

## [runtime] Entrypoint/starter not defined here; provided by orchestration
## [universal] Entrypoint wrapper defines behavior: no args → start-notebook.sh; args → exec command (runtime mode).
